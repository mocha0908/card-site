import argparse
import csv
import math
import os
import platform
import random
import stat
import sys
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple
from urllib.parse import urlencode

from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.common.exceptions import TimeoutException, WebDriverException
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait
from webdriver_manager.chrome import ChromeDriverManager
from webdriver_manager.core.driver_cache import DriverCacheManager


# =========================
# è¨­å®šç³»ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
# =========================
DEFAULT_WAIT_SEC = 12
DEFAULT_RETRY = 2
DEFAULT_DELAY = 1.0  # ãƒšãƒ¼ã‚¸é–“å¾…æ©Ÿï¼ˆç§’ï¼‰
USER_AGENT = (
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
    "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36"
)


@dataclass
class Args:
    mode: str
    group_id: Optional[int]
    keyword: str
    start_page: int
    end_page: Optional[int]
    all_pages: bool
    output: Path
    csv_mode: str  # new|append|overwrite
    headful: bool
    delay: float
    retry: int
    wait_sec: int
    rpm: Optional[int]  # requests per minute çš„ãªä¸Šé™ã€‚Noneãªã‚‰ç„¡åˆ¶é™
    checkpoint_every: int  # ä½•ãƒšãƒ¼ã‚¸ã”ã¨ã«ä¸­é–“ä¿å­˜ã™ã‚‹ã‹ï¼ˆ0ã¯ã—ãªã„ï¼‰
    reset_session_every: int  # ä½•ãƒšãƒ¼ã‚¸ã”ã¨ã«ãƒ–ãƒ©ã‚¦ã‚¶ã‚’å†èµ·å‹•ã™ã‚‹ã‹ï¼ˆ0ã¯ã—ãªã„ï¼‰


# =========================
# ãƒ‰ãƒ©ã‚¤ãƒç”Ÿæˆï¼ˆä¿®æ­£ç‰ˆï¼‰
# =========================
def find_chromedriver_executable(base_path: str) -> str:
    """
    macOSã§webdriver-managerãŒé–“é•ã£ãŸãƒ‘ã‚¹ã‚’è¿”ã™å•é¡Œã‚’ä¿®æ­£
    æ­£ã—ã„chromedriverå®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
    """
    base_dir = Path(base_path).parent

    # å¯èƒ½ãªchromedriverã®ãƒ•ã‚¡ã‚¤ãƒ«å
    possible_files = [
        "chromedriver",
        "chromedriver-mac-arm64",
        "chromedriver-mac-x64",
        "chromedriver.exe"
    ]

    for filename in possible_files:
        candidate = base_dir / filename
        if candidate.exists() and candidate.is_file():
            # å®Ÿè¡Œå¯èƒ½ãƒ•ã‚¡ã‚¤ãƒ«ã‹ç¢ºèªï¼ˆUnixç³»ã®å ´åˆï¼‰
            if platform.system() != "Windows":
                st = candidate.stat()
                if st.st_mode & stat.S_IXUSR:
                    return str(candidate)
            else:
                # Windowsã®å ´åˆã¯.exeãƒ•ã‚¡ã‚¤ãƒ«ã‚’å„ªå…ˆ
                if filename.endswith(".exe"):
                    return str(candidate)

    # è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯å…ƒã®ãƒ‘ã‚¹ã‚’è¿”ã™
    return base_path


def make_driver(headful: bool) -> webdriver.Chrome:
    options = Options()
    if not headful and not os.environ.get("HEADFUL"):
        options.add_argument("--headless=new")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")  # è¿½åŠ ï¼šå…±æœ‰ãƒ¡ãƒ¢ãƒªå•é¡Œå¯¾ç­–
    options.add_argument("--window-size=1280,2000")
    options.add_argument("--lang=ja-JP")
    options.add_argument(f"--user-agent={USER_AGENT}")
    # ã‚µã‚¤ãƒ¬ãƒ³ãƒˆåŒ–ï¼ˆä½™è¨ˆãªãƒ­ã‚°æŠ‘åˆ¶ï¼‰
    options.add_experimental_option("excludeSwitches", ["enable-automation", "enable-logging"])
    options.add_experimental_option("useAutomationExtension", False)

    # ChromeDriverã®ãƒ‘ã‚¹ã‚’å–å¾—
    driver_path = None

    # 1. ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã®chromedriverã‚’å„ªå…ˆ
    local_driver = Path(__file__).parent / "chromedriver"
    if local_driver.exists() and local_driver.is_file():
        # å®Ÿè¡Œæ¨©é™ãŒã‚ã‚‹ã‹ç¢ºèª
        st = local_driver.stat()
        if platform.system() != "Windows" and not (st.st_mode & stat.S_IXUSR):
            print(f"âš ï¸ {local_driver} ã«å®Ÿè¡Œæ¨©é™ãŒã‚ã‚Šã¾ã›ã‚“ã€‚æ¨©é™ã‚’ä»˜ä¸ã—ã¾ã™...")
            local_driver.chmod(st.st_mode | stat.S_IXUSR)
        driver_path = str(local_driver)
        print(f"Using local ChromeDriver: {driver_path}")

    # 2. ãƒ­ãƒ¼ã‚«ãƒ«ã«ãªã‘ã‚Œã°webdriver-managerã‚’ä½¿ç”¨
    if not driver_path:
        try:
            driver_path = ChromeDriverManager().install()
            # macOSã®å ´åˆã€æ­£ã—ã„å®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æ¢ã™
            if platform.system() == "Darwin":
                driver_path = find_chromedriver_executable(driver_path)
            print(f"Using ChromeDriver from webdriver-manager: {driver_path}")
        except Exception as e:
            print(f"âŒ ChromeDriverã®è‡ªå‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã«å¤±æ•—: {e}")
            print("æ‰‹å‹•ã§ChromeDriverã‚’ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã«é…ç½®ã—ã¦ãã ã•ã„ã€‚")
            print("å‚è€ƒ: https://chromedriver.chromium.org/downloads")
            raise

    try:
        driver = webdriver.Chrome(service=Service(driver_path), options=options)
    except Exception as e:
        print(f"âŒ ChromeDriverã®èµ·å‹•ã«å¤±æ•—: {e}")
        raise

    return driver


# =========================
# URLãƒ“ãƒ«ãƒ€
# =========================
def build_group_url(group_id: int, page: int) -> str:
    base = f"https://www.cardrush-pokemon.jp/product-group/{group_id}"
    return f"{base}?page={page}"


def build_search_url(page: int, keyword: str, num: int = 100, img: int = 160) -> str:
    base = "https://www.cardrush-pokemon.jp/product-list"
    query = {"keyword": keyword, "Submit": "æ¤œç´¢", "num": str(num), "img": str(img), "page": page}
    return f"{base}?{urlencode(query)}"


# =========================
# ãƒ‘ãƒ¼ã‚¹è£œåŠ©
# =========================
def extract_text(soup: BeautifulSoup, selector: str, default: str = "") -> str:
    el = soup.select_one(selector)
    return el.get_text(strip=True) if el else default


def extract_image_url(soup: BeautifulSoup) -> str:
    img = soup.select_one("img")
    if not img:
        return ""
    return img.get("data-x2") or img.get("src", "") or ""


def extract_product_id(soup: BeautifulSoup) -> str:
    # 1) input.open_modal_window_product_form ã® data-id
    tag = soup.select_one("input.open_modal_window_product_form")
    if tag and tag.has_attr("data-id") and tag["data-id"]:
        return tag["data-id"].strip()
    # 2) div.item_data ã® data-product-id
    div_tag = soup.select_one("div.item_data")
    if div_tag and div_tag.has_attr("data-product-id") and div_tag["data-product-id"]:
        return div_tag["data-product-id"].strip()
    # 3) a.item_data_link ã® href ã‹ã‚‰æ¨å®š
    link_tag = soup.select_one("a.item_data_link")
    if link_tag and link_tag.has_attr("href"):
        href = link_tag["href"]
        if "/product/" in href:
            return href.split("/product/")[1].split("?")[0].strip()
    return ""


def parse_listing_li(li_html: str) -> List[str]:
    soup = BeautifulSoup(li_html, "html.parser")
    name_text = extract_text(soup, "span.goods_name")
    price_text = extract_text(soup, "p.selling_price span.figure")
    pack_code = extract_text(soup, "span.model_number_value")
    stock_text = extract_text(soup, "p.stock").replace("åœ¨åº«æ•°ï¼š", "")
    image_url = extract_image_url(soup)
    product_id = extract_product_id(soup)
    product_url = f"https://www.cardrush-pokemon.jp/product/detail/{product_id}" if product_id else ""
    return [name_text, price_text, pack_code, stock_text, image_url, product_id, product_url]


# =========================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# =========================
def ensure_parent(path: Path) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)


def write_csv(path: Path, rows: List[List[str]], mode: str = "new") -> None:
    """
    mode: new|append|overwrite
      - new: æ—¢å­˜ãªã‚‰ã‚¨ãƒ©ãƒ¼ï¼ˆä¸Šæ›¸ãã—ãªã„ï¼‰
      - append: è¿½è¨˜ï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼ã¯ç„¡ã„å ´åˆã®ã¿æ›¸ãï¼‰
      - overwrite: å¸¸ã«ä¸Šæ›¸ãï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼æ›¸ãï¼‰
    """
    ensure_parent(path)
    header = ["å•†å“å", "ä¾¡æ ¼", "ãƒ‘ãƒƒã‚¯ç•ªå·", "åœ¨åº«æ•°", "ç”»åƒURL", "å•†å“ID", "å•†å“URL"]

    if mode == "new" and path.exists():
        raise FileExistsError(f"{path} ã¯æ—¢ã«å­˜åœ¨ã—ã¾ã™ã€‚--csv-mode append ã¾ãŸã¯ overwrite ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚")

    write_header = True
    open_mode = "w"
    if mode == "append" and path.exists():
        write_header = False
        open_mode = "a"
    elif mode == "append":
        open_mode = "w"
    elif mode == "overwrite":
        open_mode = "w"

    with open(path, open_mode, newline="", encoding="utf-8-sig") as f:
        w = csv.writer(f)
        if write_header:
            w.writerow(header)
        w.writerows(rows)


def discover_total_pages(driver: webdriver.Chrome, wait: WebDriverWait, first_url: str) -> int:
    """
    ãƒšãƒ¼ã‚¸ãƒ£ã‹ã‚‰ç·ãƒšãƒ¼ã‚¸æ•°ã‚’æ¨å®šã€‚ãªã‘ã‚Œã°1ã€‚
    """
    driver.get(first_url)
    try:
        wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, "li.list_item_cell")))
    except TimeoutException:
        # å•†å“ãŒ0ã§ã‚‚ãƒšãƒ¼ã‚¸ãƒ£ã¯å‡ºã¦ã“ãªã„å¯èƒ½æ€§ã‚ã‚Š â†’ 1æ‰±ã„
        return 1

    # ãƒšãƒ¼ã‚¸ãƒ£å€™è£œã‚’æ¢ã™
    html = driver.page_source
    soup = BeautifulSoup(html, "html.parser")
    # ã‚ˆãã‚ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³: .pagination å†…ã® a ã®æœ€å¤§ãƒšãƒ¼ã‚¸ç•ªå·
    page_nums: List[int] = []
    for a in soup.select("ul.pagination a"):
        try:
            page_nums.append(int(a.get_text(strip=True)))
        except Exception:
            continue

    return max(page_nums) if page_nums else 1


def rate_limit(last_time: List[float], rpm: Optional[int]) -> None:
    """
    1åˆ†ã‚ãŸã‚Šã®æœ€å¤§å®Ÿè¡Œå›æ•°ï¼ˆrpmï¼‰ã‚’æ“¬ä¼¼çš„ã«åˆ¶é™ã€‚
    """
    if not rpm:
        return
    min_interval = 60.0 / float(rpm)
    now = time.time()
    elapsed = now - last_time[0]
    if elapsed < min_interval:
        time.sleep(min_interval - elapsed)
    last_time[0] = time.time()


# =========================
# ã‚³ã‚¢å‡¦ç†ï¼ˆâ˜… ãƒªãƒˆãƒ©ã‚¤æ™‚ã‚»ãƒƒã‚·ãƒ§ãƒ³å†ç”Ÿæˆã‚’çµ„ã¿è¾¼ã¿ï¼‰
# =========================
def scrape_pages(
    args: Args,
    driver: webdriver.Chrome,
    wait: WebDriverWait,
) -> Tuple[List[List[str]], webdriver.Chrome]:
    rows: List[List[str]] = []
    seen_ids: Set[str] = set()

    # é–‹å§‹ãƒ»çµ‚äº†ãƒšãƒ¼ã‚¸ã®æ±ºå®š
    if args.mode == "group":
        assert args.group_id is not None, "mode=group ã§ã¯ --group-id ãŒå¿…é ˆã§ã™ã€‚"
        first_url = build_group_url(args.group_id, args.start_page)
    else:
        first_url = build_search_url(args.start_page, keyword=args.keyword)

    if args.all_pages:
        total_pages = discover_total_pages(driver, wait, first_url)
        start_page = 1
        end_page = total_pages
        print(f"ğŸ” ç·ãƒšãƒ¼ã‚¸æ•°ã‚’è‡ªå‹•æ¤œå‡º: {total_pages} ãƒšãƒ¼ã‚¸")
    else:
        start_page = args.start_page
        end_page = args.end_page if args.end_page is not None else args.start_page

    # é€²æ—ã®ã–ã£ãã‚Šè¡¨ç¤ºç”¨
    total_steps = max(1, end_page - start_page + 1)
    width = len(str(end_page))
    next_checkpoint_at = start_page + (args.checkpoint_every or 0)

    last_call = [0.0]  # rate limitç®¡ç†

    for page in range(start_page, end_page + 1):
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒªã‚»ãƒƒãƒˆï¼ˆä¸€å®šãƒšãƒ¼ã‚¸ã”ã¨ã«ãƒ–ãƒ©ã‚¦ã‚¶å†èµ·å‹•ï¼‰
        if args.reset_session_every and (page - start_page) > 0 and (page - start_page) % args.reset_session_every == 0:
            print(f"ğŸ”„ ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒªã‚»ãƒƒãƒˆï¼ˆ{args.reset_session_every}ãƒšãƒ¼ã‚¸ã”ã¨ï¼‰...")
            try:
                driver.quit()
            except Exception:
                pass
            time.sleep(random.uniform(3.0, 6.0))  # å°‘ã—é•·ã‚ã«ä¼‘æ†©
            driver = make_driver(headful=args.headful)
            wait = WebDriverWait(driver, args.wait_sec)
            print("  â†’ ãƒ–ãƒ©ã‚¦ã‚¶å†èµ·å‹•å®Œäº†")

        # ãƒ¬ãƒ¼ãƒˆåˆ¶å¾¡
        rate_limit(last_call, args.rpm)

        # URLæ§‹ç¯‰
        if args.mode == "group":
            url = build_group_url(args.group_id, page)
        else:
            url = build_search_url(page, keyword=args.keyword)

        # ãƒªãƒˆãƒ©ã‚¤è¾¼ã¿ã§å–å¾—ï¼ˆâ˜…å¤±æ•—æ™‚ã¯ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œã‚Šç›´ã™ï¼‰
        last_err: Optional[Exception] = None
        for attempt in range(1, args.retry + 2):
            try:
                print(f"[{page:>{width}}/{end_page}] GET {url}  (try {attempt}/{args.retry + 1})")
                driver.get(url)
                wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, "li.list_item_cell")))
                time.sleep(0.2)  # è»½ã„æç”»å¾…ã¡

                items = driver.find_elements(By.CSS_SELECTOR, "li.list_item_cell")
                added_page = 0
                for it in items:
                    li_html = it.get_attribute("outerHTML")
                    row = parse_listing_li(li_html)

                    pid = row[5]
                    if pid and pid in seen_ids:
                        continue
                    if pid:
                        seen_ids.add(pid)

                    rows.append(row)
                    added_page += 1

                print(f"  â†’ {added_page} ä»¶")
                break  # æˆåŠŸ
            except Exception as e:
                last_err = e
                print(f"  âš ï¸ ã‚¨ãƒ©ãƒ¼: {repr(e)}")

                # â˜… ã“ã“ãŒè¿½åŠ ç‚¹ï¼šãƒªãƒˆãƒ©ã‚¤å‰ã«ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ãƒªã‚»ãƒƒãƒˆ
                try:
                    driver.quit()
                except Exception:
                    pass
                # çŸ­ã„ãƒãƒƒã‚¯ã‚ªãƒ•ï¼‹æ–°ã‚»ãƒƒã‚·ãƒ§ãƒ³
                backoff = 1.0 + 0.5 * attempt + random.uniform(0, 0.5)
                time.sleep(backoff)
                try:
                    driver = make_driver(headful=args.headful)
                    wait = WebDriverWait(driver, args.wait_sec)
                    print("  â†» æ–°ã—ã„ãƒ–ãƒ©ã‚¦ã‚¶ã‚»ãƒƒã‚·ãƒ§ãƒ³ã§å†è©¦è¡Œã—ã¾ã™")
                except Exception as boot_e:
                    print(f"  âŒ ã‚»ãƒƒã‚·ãƒ§ãƒ³å†ç”Ÿæˆã«å¤±æ•—: {repr(boot_e)}")
                    # ã•ã‚‰ã«å¾…ã£ã¦æ¬¡ã®attemptã¸ï¼ˆæ¬¡ã®ãƒ«ãƒ¼ãƒ—ã§ã¾ãŸå†ç”Ÿæˆã‚’è©¦ã¿ã‚‹ï¼‰
                    time.sleep(1.5)

                if attempt > args.retry:
                    print(f"  âŒ ãƒšãƒ¼ã‚¸ {page} ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆæœ€çµ‚ã‚¨ãƒ©ãƒ¼ï¼‰: {repr(last_err)}")

        # ãƒšãƒ¼ã‚¸é–“ã®å¾…æ©Ÿï¼ˆã‚†ã‚‹ãƒ©ãƒ³ãƒ€ãƒ ï¼‰
        time.sleep(args.delay + random.uniform(0, 0.4))

        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ï¼ˆâ˜… æ‹¬å¼§ã§å„ªå…ˆé †ä½ã‚’æ˜ç¤ºï¼‰
        if args.checkpoint_every and (
            page == next_checkpoint_at or ((page - start_page + 1) % args.checkpoint_every == 0)
        ):
            try:
                print("ğŸ’¾ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜â€¦")
                write_csv(args.output, rows, mode="overwrite")
                print("  â†’ ä¿å­˜å®Œäº†")
            except Exception as e:
                print(f"  âš ï¸ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜å¤±æ•—: {e}")
            next_checkpoint_at = page + args.checkpoint_every

    return rows, driver


# =========================
# å¼•æ•°å‡¦ç†
# =========================
def parse_args() -> Args:
    p = argparse.ArgumentParser(description="CardRush Pokemon listing scraper (usability enhanced)")
    p.add_argument("--mode", choices=["group", "search"], required=True, help="group or search")
    p.add_argument("--group-id", type=int, help="product-group/<ID> ã®IDï¼ˆmode=groupæ™‚å¿…é ˆï¼‰")
    p.add_argument("--keyword", type=str, default="", help="æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆmode=searchæ™‚æœ‰åŠ¹ï¼‰")

    p.add_argument("--start-page", type=int, default=1)
    p.add_argument("--end-page", type=int, help="æœªæŒ‡å®šãªã‚‰ start-page ã®ã¿")
    p.add_argument("--all-pages", action="store_true", help="ãƒšãƒ¼ã‚¸æ•°ã‚’è‡ªå‹•æ¤œå‡ºã—ã¦å…¨ãƒšãƒ¼ã‚¸ã‚’å¯¾è±¡ã«ã™ã‚‹")

    p.add_argument("--output", type=Path, required=True, help="å‡ºåŠ›CSVãƒ•ã‚¡ã‚¤ãƒ«å")
    p.add_argument("--csv-mode", choices=["new", "append", "overwrite"], default="new",
                   help="CSVã®å‡ºåŠ›ãƒ¢ãƒ¼ãƒ‰: new(æ—¢å­˜ãªã‚‰ã‚¨ãƒ©ãƒ¼)/append(è¿½è¨˜)/overwrite(ä¸Šæ›¸ã)")

    p.add_argument("--headful", action="store_true", help="ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ç„¡åŠ¹åŒ–ï¼ˆãƒ–ãƒ©ã‚¦ã‚¶ã‚’è¡¨ç¤ºï¼‰")
    p.add_argument("--delay", type=float, default=DEFAULT_DELAY, help="ãƒšãƒ¼ã‚¸é–“ã®å¾…æ©Ÿç§’ï¼ˆãƒ©ãƒ³ãƒ€ãƒ å¾®å¢—ã‚ã‚Šï¼‰")
    p.add_argument("--retry", type=int, default=DEFAULT_RETRY, help="ãƒšãƒ¼ã‚¸å–å¾—ã®æœ€å¤§å†è©¦è¡Œå›æ•°")
    p.add_argument("--wait-sec", type=int, default=DEFAULT_WAIT_SEC, help="DOMå¾…æ©Ÿã®æœ€å¤§ç§’æ•°")
    p.add_argument("--rpm", type=int, help="1åˆ†ã‚ãŸã‚Šã®æœ€å¤§ã‚¢ã‚¯ã‚»ã‚¹æ•°ï¼ˆãƒ¬ãƒ¼ãƒˆåˆ¶å¾¡ï¼‰")
    p.add_argument("--checkpoint-every", type=int, default=0,
                   help="æŒ‡å®šãƒšãƒ¼ã‚¸ã”ã¨ã«ä¸­é–“ä¿å­˜ï¼ˆ0=ã—ãªã„ï¼‰")
    p.add_argument("--reset-session-every", type=int, default=0,
                   help="æŒ‡å®šãƒšãƒ¼ã‚¸ã”ã¨ã«ãƒ–ãƒ©ã‚¦ã‚¶ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ãƒªã‚»ãƒƒãƒˆï¼ˆ0=ã—ãªã„ã€æ¨å¥¨: 15-20ï¼‰")

    a = p.parse_args()

    # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ & ä½¿ã„å‹æ‰‹å‘ä¸Š
    if a.mode == "group" and a.group_id is None:
        p.error("mode=group ã§ã¯ --group-id ãŒå¿…é ˆã§ã™ã€‚")

    if a.all_pages and a.end_page is not None:
        print("â„¹ï¸ --all-pages ãŒæŒ‡å®šã•ã‚ŒãŸãŸã‚ --end-page ã¯ç„¡è¦–ã—ã¾ã™ã€‚", file=sys.stderr)

    return Args(
        mode=a.mode,
        group_id=a.group_id,
        keyword=a.keyword,
        start_page=a.start_page,
        end_page=a.end_page,
        all_pages=a.all_pages,
        output=a.output,
        csv_mode=a.csv_mode,
        headful=a.headful,
        delay=a.delay,
        retry=a.retry,
        wait_sec=a.wait_sec,
        rpm=a.rpm,
        checkpoint_every=a.checkpoint_every,
        reset_session_every=a.reset_session_every,
    )


# =========================
# ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆï¼ˆâ˜… æœ€çµ‚ãƒ‰ãƒ©ã‚¤ãƒã‚’ç¢ºå®Ÿã«quitï¼‰
# =========================
def main() -> int:
    args = parse_args()
    driver = make_driver(headful=args.headful)
    wait = WebDriverWait(driver, args.wait_sec)

    # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã«è¿½è¨˜/ä¸Šæ›¸ãã®å‰æº–å‚™
    if args.csv_mode == "overwrite" and args.output.exists():
        # ä¸Šæ›¸ãé–‹å§‹å‰ã«ãƒ˜ãƒƒãƒ€ã§åˆæœŸåŒ–
        write_csv(args.output, [], mode="overwrite")
    elif args.csv_mode == "new" and args.output.exists():
        print(f"âŒ {args.output} ã¯æ—¢ã«å­˜åœ¨ã—ã¾ã™ã€‚--csv-mode append/overwrite ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚")
        try:
            driver.quit()
        except Exception:
            pass
        return 2
    elif args.csv_mode == "append" and not args.output.exists():
        # appendã§ã‚‚åˆå›ã¯ãƒ˜ãƒƒãƒ€ä»˜ãã§ä½œæˆ
        write_csv(args.output, [], mode="append")

    try:
        rows, driver = scrape_pages(args, driver, wait)  # â˜… æœ€æ–°driverã‚’å—ã‘å–ã‚‹
    finally:
        # â˜… scrape_pages å†…ã§æ–°è¦ç”Ÿæˆã—ãŸdriverãŒã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€å¿…ãšæœ€å¾Œã®å‚ç…§ã§quit
        try:
            driver.quit()
        except Exception:
            pass

    # åé›†ãƒ‡ãƒ¼ã‚¿ã‚’æ›¸ãå‡ºã—
    try:
        write_csv(args.output, rows, mode="append" if args.csv_mode == "append" else "overwrite")
    except FileExistsError:
        # csv_mode=new ã§æ—¢ã«å­˜åœ¨ã—ã¦ã„ãŸã‚±ãƒ¼ã‚¹ã¯ã“ã“ã«ã¯æ¥ãªã„ã¯ãšã ãŒäºŒé‡é˜²è¡›
        print(f"âŒ {args.output} ã¯æ—¢ã«å­˜åœ¨ã—ã¾ã™ã€‚--csv-mode append/overwrite ã«ã—ã¦ãã ã•ã„ã€‚")
        return 2

    print(f"âœ… å®Œäº†: {len(rows)} ä»¶ã‚’ {args.output} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
    return 0


if __name__ == "__main__":
    sys.exit(main())