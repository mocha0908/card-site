import argparse
import csv
import math
import os
import platform
import random
import stat
import sys
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple
from urllib.parse import urlencode

from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.common.exceptions import TimeoutException, WebDriverException
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait
from webdriver_manager.chrome import ChromeDriverManager

# =========================
# è¨­å®šç³»
# =========================
DEFAULT_WAIT_SEC = 15
DEFAULT_RETRY = 2
DEFAULT_DELAY = 1.0
# ã‚·ãƒ³ã‚¿ãƒƒã‚¯ã‚¹ã‚¨ãƒ©ãƒ¼å›é¿ã®ãŸã‚1è¡Œã§è¨˜è¿°
USER_AGENT = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"

@dataclass
class Args:
    mode: str
    group_id: Optional[int]
    keyword: str
    start_page: int
    end_page: Optional[int]
    all_pages: bool
    output: Path
    csv_mode: str
    headful: bool
    delay: float
    retry: int
    wait_sec: int
    rpm: Optional[int]
    checkpoint_every: int
    reset_session_every: int

# =========================
# ãƒ‰ãƒ©ã‚¤ãƒç”Ÿæˆ
# =========================
def find_chromedriver_executable(base_path: str) -> str:
    base_dir = Path(base_path).parent
    possible_files = ["chromedriver", "chromedriver-mac-arm64", "chromedriver-mac-x64", "chromedriver.exe"]
    for filename in possible_files:
        candidate = base_dir / filename
        if candidate.exists() and candidate.is_file():
            if platform.system() != "Windows":
                st = candidate.stat()
                if st.st_mode & stat.S_IXUSR:
                    return str(candidate)
            else:
                if filename.endswith(".exe"):
                    return str(candidate)
    return base_path

def make_driver(headful: bool) -> webdriver.Chrome:
    options = Options()
    if not headful and not os.environ.get("HEADFUL"):
        options.add_argument("--headless=new")
    
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--window-size=1280,2000")
    options.add_argument("--lang=ja-JP")
    options.add_argument(f"--user-agent={USER_AGENT}")
    
    # ãƒ­ãƒœãƒƒãƒˆæ¤œçŸ¥å›é¿
    options.add_argument('--disable-blink-features=AutomationControlled')
    options.add_experimental_option("excludeSwitches", ["enable-automation", "enable-logging"])
    options.add_experimental_option("useAutomationExtension", False)

    driver_path = None
    local_driver = Path(__file__).parent / "chromedriver"
    if local_driver.exists() and local_driver.is_file():
        st = local_driver.stat()
        if platform.system() != "Windows" and not (st.st_mode & stat.S_IXUSR):
            local_driver.chmod(st.st_mode | stat.S_IXUSR)
        driver_path = str(local_driver)

    if not driver_path:
        try:
            driver_path = ChromeDriverManager().install()
            if platform.system() == "Darwin":
                driver_path = find_chromedriver_executable(driver_path)
        except Exception:
            raise RuntimeError("ChromeDriverã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã«å¤±æ•—ã—ã¾ã—ãŸ")

    return webdriver.Chrome(service=Service(driver_path), options=options)

# =========================
# URLãƒ“ãƒ«ãƒ€
# =========================
def build_group_url(group_id: int, page: int) -> str:
    # num=100 ã‚’ä»˜ä¸ã—ã¦1ãƒšãƒ¼ã‚¸ã‚ãŸã‚Šã®å–å¾—æ•°ã‚’æœ€å¤§åŒ–
    base = f"https://www.cardrush-pokemon.jp/product-group/{group_id}"
    return f"{base}?page={page}&num=100&img=160"

def build_search_url(page: int, keyword: str, num: int = 100, img: int = 160) -> str:
    base = "https://www.cardrush-pokemon.jp/product-list"
    query = {"keyword": keyword, "Submit": "æ¤œç´¢", "num": str(num), "img": str(img), "page": page}
    return f"{base}?{urlencode(query)}"

# =========================
# ãƒ‘ãƒ¼ã‚¹å‡¦ç† (Correct Selectors)
# =========================
def extract_text(soup: BeautifulSoup, selector: str, default: str = "") -> str:
    el = soup.select_one(selector)
    return el.get_text(strip=True) if el else default

def extract_image_url(soup: BeautifulSoup) -> str:
    img = soup.select_one("img")
    if not img:
        return ""
    # é…å»¶èª­ã¿è¾¼ã¿å¯¾å¿œ: data-x2 > data-src > src
    return img.get("data-x2") or img.get("data-src") or img.get("src", "") or ""

def extract_product_id(soup: BeautifulSoup) -> str:
    # URLã‹ã‚‰IDã‚’æŠ½å‡ºã™ã‚‹ã®ãŒæœ€ã‚‚ç¢ºå®Ÿ
    link_tag = soup.select_one("a")
    if link_tag and link_tag.has_attr("href"):
        href = link_tag["href"]
        if "/product/" in href:
            try:
                return href.split("/product/")[1].split("?")[0].strip()
            except:
                pass
    return ""

def parse_listing_li(li_html: str) -> List[str]:
    # ãƒªã‚¹ãƒˆæ§‹é€  li.list_item_cell ã«å¯¾å¿œ
    soup = BeautifulSoup(li_html, "html.parser")
    
    name_text = extract_text(soup, "span.goods_name")
    price_text = extract_text(soup, "span.figure").replace(",", "")
    # stockã¯ "åœ¨åº«æ•° 5ç‚¹" ã®ã‚ˆã†ãªå½¢å¼
    stock_raw = extract_text(soup, "p.stock")
    stock_text = stock_raw.replace("åœ¨åº«æ•°", "").replace("ç‚¹", "").replace("æš", "").strip()
    
    image_url = extract_image_url(soup)
    product_id = extract_product_id(soup)
    product_url = f"https://www.cardrush-pokemon.jp/product/{product_id}" if product_id else ""
    
    return [name_text, price_text, stock_text, image_url, product_url]

# =========================
# ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†
# =========================
def write_csv(path: Path, rows: List[List[str]], mode: str = "new") -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    header = ["å•†å“å", "ä¾¡æ ¼", "åœ¨åº«æ•°", "ç”»åƒURL", "å•†å“URL"]

    write_header = True
    open_mode = "w"
    
    if mode == "append":
        if path.exists():
            write_header = False
            open_mode = "a"
    elif mode == "overwrite":
        open_mode = "w"
    
    with open(path, open_mode, newline="", encoding="utf-8-sig") as f:
        w = csv.writer(f)
        if write_header:
            w.writerow(header)
        w.writerows(rows)

# =========================
# ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚³ã‚¢
# =========================
def discover_total_pages(driver: webdriver.Chrome, wait: WebDriverWait, first_url: str) -> int:
    driver.get(first_url)
    try:
        # å•†å“ãƒªã‚¹ãƒˆãŒå‡ºã‚‹ã¾ã§å¾…ã¤ (æ­£ã—ã„ã‚»ãƒ¬ã‚¯ã‚¿)
        wait.until(EC.presence_of_element_located((By.CLASS_NAME, "list_item_cell")))
    except TimeoutException:
        # å•†å“ãŒç„¡ã„ã€ã¾ãŸã¯èª­ã¿è¾¼ã¿å¤±æ•—
        return 1

    soup = BeautifulSoup(driver.page_source, "html.parser")
    # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã‹ã‚‰æœ€å¤§ãƒšãƒ¼ã‚¸ã‚’å–å¾—
    # ä¾‹: [1] [2] ... [46] [æ¬¡ã¸]
    # ã‚«ãƒ¼ãƒ‰ãƒ©ãƒƒã‚·ãƒ¥ã®æ§‹é€ ä¾å­˜
    page_links = soup.select("div.pager a")
    max_page = 1
    for link in page_links:
        txt = link.get_text(strip=True)
        if txt.isdigit():
            p = int(txt)
            if p > max_page:
                max_page = p
    return max_page

def scrape_pages(args: Args, driver: webdriver.Chrome, wait: WebDriverWait) -> Tuple[List[List[str]], webdriver.Chrome]:
    rows: List[List[str]] = []
    
    if args.mode == "group":
        first_url = build_group_url(args.group_id, args.start_page)
    else:
        first_url = build_search_url(args.start_page, keyword=args.keyword)

    if args.all_pages:
        print("ğŸ” ç·ãƒšãƒ¼ã‚¸æ•°ã‚’ç¢ºèªä¸­...")
        total_pages = discover_total_pages(driver, wait, first_url)
        start_page = 1
        end_page = total_pages
        print(f"ğŸ” ç·ãƒšãƒ¼ã‚¸æ•°: {total_pages}")
    else:
        start_page = args.start_page
        end_page = args.end_page if args.end_page else args.start_page

    for page in range(start_page, end_page + 1):
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒªã‚»ãƒƒãƒˆ
        if args.reset_session_every and (page - start_page) > 0 and (page - start_page) % args.reset_session_every == 0:
            print("ğŸ”„ ãƒ–ãƒ©ã‚¦ã‚¶å†èµ·å‹•ä¸­...")
            driver.quit()
            time.sleep(3)
            driver = make_driver(headful=args.headful)
            wait = WebDriverWait(driver, args.wait_sec)

        # URLæ±ºå®š
        if args.mode == "group":
            url = build_group_url(args.group_id, page)
        else:
            url = build_search_url(page, keyword=args.keyword)
        
        # å–å¾—ãƒˆãƒ©ã‚¤
        for attempt in range(args.retry + 1):
            try:
                print(f"[{page}/{end_page}] å–å¾—ä¸­: {url}")
                driver.get(url)
                
                # â˜…ä¿®æ­£: æ­£ã—ã„ã‚¯ãƒ©ã‚¹å list_item_cell ã‚’å¾…æ©Ÿ
                wait.until(EC.presence_of_all_elements_located((By.CLASS_NAME, "list_item_cell")))
                
                # ç”»åƒèª­ã¿è¾¼ã¿èª˜ç™º
                driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(1.5)

                items = driver.find_elements(By.CLASS_NAME, "list_item_cell")
                if not items:
                     # å¿µã®ãŸã‚BeautifulSoupã§ã‚‚ç¢ºèª
                    soup = BeautifulSoup(driver.page_source, "html.parser")
                    if not soup.select("li.list_item_cell"):
                        print("  âš  å•†å“ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ (0ä»¶)")
                        break

                current_rows = []
                for it in items:
                    current_rows.append(parse_listing_li(it.get_attribute("outerHTML")))
                
                rows.extend(current_rows)
                print(f"  â†’ {len(current_rows)} ä»¶")
                break # æˆåŠŸ

            except Exception as e:
                print(f"  âš  ã‚¨ãƒ©ãƒ¼ (try {attempt+1}): {e}")
                time.sleep(3)
                if attempt == args.retry:
                    print("  âŒ ã“ã®ãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")

        time.sleep(args.delay + random.uniform(0, 1.0))
        
        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
        if args.checkpoint_every and page % args.checkpoint_every == 0:
            write_csv(args.output, rows, mode="overwrite")
            print(f"ğŸ’¾ ä¸­é–“ä¿å­˜å®Œäº† ({len(rows)}ä»¶)")

    return rows, driver

# =========================
# ãƒ¡ã‚¤ãƒ³
# =========================
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--mode", choices=["group", "search"], required=True)
    parser.add_argument("--group-id", type=int)
    parser.add_argument("--keyword", type=str, default="")
    parser.add_argument("--start-page", type=int, default=1)
    parser.add_argument("--end-page", type=int)
    parser.add_argument("--all-pages", action="store_true")
    parser.add_argument("--output", type=Path, required=True)
    parser.add_argument("--csv-mode", choices=["new", "append", "overwrite"], default="new")
    parser.add_argument("--headful", action="store_true")
    parser.add_argument("--delay", type=float, default=DEFAULT_DELAY)
    parser.add_argument("--retry", type=int, default=DEFAULT_RETRY)
    parser.add_argument("--wait-sec", type=int, default=DEFAULT_WAIT_SEC)
    parser.add_argument("--rpm", type=int)
    parser.add_argument("--checkpoint-every", type=int, default=0)
    parser.add_argument("--reset-session-every", type=int, default=0)

    args_parsed = parser.parse_args()
    
    # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    if args_parsed.mode == "group" and not args_parsed.group_id:
        parser.error("--group-id is required for group mode")

    args = Args(**vars(args_parsed))
    
    driver = make_driver(headful=args.headful)
    wait = WebDriverWait(driver, args.wait_sec)

    try:
        # ãƒ˜ãƒƒãƒ€ãƒ¼åˆæœŸåŒ–(overwrite/newã®å ´åˆ)
        if args.csv_mode in ["overwrite", "new"]:
            if args.csv_mode == "new" and args.output.exists():
                print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã™: {args.output}")
                return
            write_csv(args.output, [], mode=args.csv_mode)

        rows, driver = scrape_pages(args, driver, wait)
        
        # æœ€çµ‚ä¿å­˜ (appendãªã‚‰è¿½è¨˜)
        save_mode = "append" if args.csv_mode == "append" else "overwrite"
        write_csv(args.output, rows, mode=save_mode)
        print(f"ğŸ‰ å…¨å®Œäº†: {len(rows)} ä»¶ã‚’ä¿å­˜ã—ã¾ã—ãŸ")

    finally:
        driver.quit()

if __name__ == "__main__":
    main()
